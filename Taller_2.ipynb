{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe442f04-1ddd-47d6-a2a6-ac3f79b8f8b8",
   "metadata": {},
   "source": [
    "Actividad 1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "081f2266-3847-455e-af80-57246eae331a",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de datos, llevamos a cabo una limpieza de columnas y rellenamos los datos perdidos con ceros, esta última decisión la tomamos al ser que la columna INS_1SEM e INS_2SEM proporciona información de las asignaturas que se tomaron por cada semestre cada alumno, lo que nos llevó a conjeturar que los promedios que se encontraban vacíos es porque el estudiante no los inscribió por lo que nos pareció más prudente reemplazarlos por ceros que por la media, la cual era otra de nuestras opciones.\n",
    "A continuación mencionamos las columnas que determinamos que no son relevantes para este objetivo del taller: ACTF_1SEM_R, ACTF_2SEM_R, descripcion_situacion_egreso_postulante, nombre_secretaria_admision, descripcion_jefe_familia, descripcion_nivel_educacion_padre, descripcion_nivel_educacion_madre, descipcion_tipo_organismo_trabajan_padre, descripcion_tipo_organismo_trabajan_madre, descripcion_ocupacion_principal_padre, descripcion_ocupacion_principal_madre, descripcion_rama_actividad_padre, descripcion_rama_actividad_madre, cuantos_trabajan_grupos_familiar, cuantos_estudian_grupo_familiar, cuantos_estudian_grupo_pre_basica, cuantos_estudian_grupo_media_1_3, cuantos_estudian_grupo_media_4, cuantos_estudian_grupo_otras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383e324-cd03-423f-b21a-51a61083c8d9",
   "metadata": {},
   "source": [
    "Actividad 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff445bd-6fad-4c57-8e28-201ee06d3c63",
   "metadata": {},
   "source": [
    "En este caso se usó <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\">KFold</a>. Recuerde que también existe <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48ae95bb-9df7-4906-969f-dcb8535fa386",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#Entrenar el modelo\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#Evaluar el modelo\u001b[39;00m\n\u001b[0;32m     56\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Vania\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Vania\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "#Cargar los datos desde el archivo CSV\n",
    "archivo = 'Taller_2_Titulacion_DatosTaller.csv'\n",
    "datos = pd.read_csv(archivo, encoding='latin-1', delimiter=';')\n",
    "\n",
    "#Separar las características y la variable objetivo\n",
    "X = datos.iloc[:, :-1]\n",
    "y = datos.iloc[:, -1]\n",
    "\n",
    "#Eliminar la columna \"Id\"\n",
    "X = X.drop('Id', axis=1)\n",
    "\n",
    "#Convertir variables categóricas en dummies\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "#Convertir las etiquetas categóricas en valores numéricos\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "#Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "'''\n",
    "Mostrar las primeras filas de los conjuntos de datos\n",
    "print(\"Conjunto de entrenamiento:\")\n",
    "display(X_train.head())\n",
    "display(y_train.head())\n",
    "\n",
    "print(\"\\nConjunto de prueba:\")\n",
    "display(X_test.head())\n",
    "display(y_test.head())\n",
    "\n",
    "Crear el modelo de red neuronal\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))  # Capa de entrada\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))  # Capa de salida con dos neuronas para clasificación binaria con función softmax\n",
    "\n",
    "#Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Pérdida en el conjunto de prueba: {loss:.4f}')\n",
    "print(f'Precisión en el conjunto de prueba: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442b819-9c43-4467-a6cd-aeebd90bbe4b",
   "metadata": {},
   "source": [
    "Actividad 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3fc93-72fd-401c-b56f-1a585fc7bf3b",
   "metadata": {},
   "source": [
    "En este caso debieran que ser 4 bloques 2 modelos y dos modificaciones de parametros para cada modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b518c24-7647-4a46-85d5-89387bd720d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vania\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor: 0.705051862839014 usando {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.29270825126995803, 'model__learning_rate': 0.00025966252220214196, 'model__neurons': 17}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Evaluar el mejor modelo en el conjunto de prueba\u001b[39;00m\n\u001b[0;32m     75\u001b[0m best_model \u001b[38;5;241m=\u001b[39m random_search_result\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m---> 76\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPérdida en el conjunto de prueba: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecisión en el conjunto de prueba: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Cargar los datos desde el archivo CSV\n",
    "archivo = 'Taller_2_Titulacion_DatosTaller.csv'\n",
    "datos = pd.read_csv(archivo, encoding='latin-1', delimiter=';')\n",
    "\n",
    "# Ajustar los nombres de las columnas\n",
    "datos.rename(columns={'Rotulo_Titulado': 'Rotulo_Titulo', 'Rotulo_Titulo_Si': 'Rotulo_Titulo_SI'}, inplace=True)\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = datos.drop(['Id', 'Rotulo_Titulo'], axis=1)  # Eliminar 'Id' y 'Rotulo_Titulo' del conjunto de características\n",
    "y = datos['Rotulo_Titulo']  # Variable objetivo\n",
    "\n",
    "# Convertir variables categóricas en dummies\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Convertir las etiquetas categóricas en valores numéricos\n",
    "y = pd.get_dummies(y, drop_first=True)  # Convertir a numérico y evitar la trampa de la variable ficticia\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convertir los datos a tipo float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Definir una función para crear el modelo Keras\n",
    "def create_model(neurons=32, dropout_rate=0.5, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Crear el clasificador Keras\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Definir la distribución de parámetros para la búsqueda aleatoria\n",
    "param_dist = {\n",
    "    'model__neurons': randint(16, 128),\n",
    "    'model__dropout_rate': uniform(0.1, 0.5),\n",
    "    'model__learning_rate': uniform(0.0001, 0.01),\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda aleatoria\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "random_search_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores parámetros y la mejor precisión\n",
    "print(f'Mejor: {random_search_result.best_score_} usando {random_search_result.best_params_}')\n",
    "\n",
    "# Evaluar el mejor modelo en el conjunto de prueba\n",
    "best_model = random_search_result.best_estimator_.model\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Pérdida en el conjunto de prueba: {loss:.4f}')\n",
    "print(f'Precisión en el conjunto de prueba: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad064ee-0a4f-479f-8243-fb0097a4d5a5",
   "metadata": {},
   "source": [
    "Actividad 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cb731-1c83-415c-8980-443d59351006",
   "metadata": {},
   "source": [
    "Aplicacion del modelo a nuevo conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c22dd546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vania\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5111 - loss: 0.7262 - val_accuracy: 0.6915 - val_loss: 0.5909\n",
      "Epoch 2/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6527 - loss: 0.6065 - val_accuracy: 0.7101 - val_loss: 0.5267\n",
      "Epoch 3/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6718 - loss: 0.5809 - val_accuracy: 0.7164 - val_loss: 0.5131\n",
      "Epoch 4/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7194 - loss: 0.5543 - val_accuracy: 0.7184 - val_loss: 0.5150\n",
      "Epoch 5/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7252 - loss: 0.5408 - val_accuracy: 0.7267 - val_loss: 0.5105\n",
      "Epoch 6/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7255 - loss: 0.5438 - val_accuracy: 0.7226 - val_loss: 0.5132\n",
      "Epoch 7/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7333 - loss: 0.5556 - val_accuracy: 0.7267 - val_loss: 0.5082\n",
      "Epoch 8/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7586 - loss: 0.5272 - val_accuracy: 0.7308 - val_loss: 0.5053\n",
      "Epoch 9/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7464 - loss: 0.5178 - val_accuracy: 0.7267 - val_loss: 0.5083\n",
      "Epoch 10/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7661 - loss: 0.5107 - val_accuracy: 0.7371 - val_loss: 0.5083\n",
      "Epoch 11/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7731 - loss: 0.4950 - val_accuracy: 0.7267 - val_loss: 0.5073\n",
      "Epoch 12/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7737 - loss: 0.4835 - val_accuracy: 0.7308 - val_loss: 0.5128\n",
      "Epoch 13/50\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7764 - loss: 0.4840 - val_accuracy: 0.7288 - val_loss: 0.5140\n",
      "Loss: 0.5322\n",
      "Accuracy: 0.7169\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "archivo = 'Taller_2_Titulacion_DatosTaller.csv'\n",
    "datos = pd.read_csv(archivo, encoding='latin-1', delimiter=';')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = datos.iloc[:, :-1]\n",
    "y = datos.iloc[:, -1]\n",
    "\n",
    "# Drop the \"Id\" column\n",
    "X = X.drop('Id', axis=1)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', StandardScaler())  # Scale features\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Preprocess the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert target variable to numerical (binary encoding)\n",
    "y = pd.get_dummies(y, drop_first=True).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(32, activation='relu'))  # Hidden layer 1\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(16, activation='relu'))  # Hidden layer 2\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ac4a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vania\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "archivo = 'Taller_2_Titulacion_DatosTaller.csv'\n",
    "datos = pd.read_csv(archivo, encoding='latin-1', delimiter=';')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = datos.iloc[:, :-1]\n",
    "y = datos.iloc[:, -1]\n",
    "\n",
    "# Drop the \"Id\" column\n",
    "X = X.drop('Id', axis=1)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Change to median imputation\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Preprocess the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert target variable to numerical (binary encoding)\n",
    "y = pd.get_dummies(y, drop_first=True).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.4))  # Adjust dropout rate\n",
    "model.add(Dense(128, activation='relu'))  # Increase layer complexity\n",
    "model.add(Dropout(0.4))  # Adjust dropout rate\n",
    "model.add(Dense(64, activation='relu'))  # Add another hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3bcbb653-a21e-41f5-969a-1bc519c56b2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- nombre_comuna_EM_CALBUCO\n- nombre_comuna_EM_CERRO NAVIA\n- nombre_comuna_EM_FREIRINA\n- nombre_comuna_EM_LAMPA\n- nombre_comuna_EM_PAILLACO\n- ...\nFeature names seen at fit time, yet now missing:\n- descripcion_nivel_educacion_madre_Instituto profesional completa\n- descripcion_nivel_educacion_madre_Sin estudios\n- descripcion_ocupacion_principal_madre_Obrero agrícola o minero calificado& mecánico& electricista&  tractorista& etc.\n- descripcion_rama_actividad_madre_Explotación de minas y canteras\n- descripcion_rama_educacional_EM_Sin Datos\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m X_evaluacion \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X_evaluacion)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Hacer predicciones con el modelo cargado\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_evaluacion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Convertir las predicciones a etiquetas legibles\u001b[39;00m\n\u001b[0;32m     16\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(predictions[:, \u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSÍ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Si la segunda columna (índice 1) es True, entonces 'SÍ', de lo contrario 'NO'\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:500\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    499\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 500\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    502\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:460\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    468\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[0;32m    469\u001b[0m ):\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    517\u001b[0m ):\n\u001b[0;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    503\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[1;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- nombre_comuna_EM_CALBUCO\n- nombre_comuna_EM_CERRO NAVIA\n- nombre_comuna_EM_FREIRINA\n- nombre_comuna_EM_LAMPA\n- nombre_comuna_EM_PAILLACO\n- ...\nFeature names seen at fit time, yet now missing:\n- descripcion_nivel_educacion_madre_Instituto profesional completa\n- descripcion_nivel_educacion_madre_Sin estudios\n- descripcion_ocupacion_principal_madre_Obrero agrícola o minero calificado& mecánico& electricista&  tractorista& etc.\n- descripcion_rama_actividad_madre_Explotación de minas y canteras\n- descripcion_rama_educacional_EM_Sin Datos\n- ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de evaluación\n",
    "datos_evaluacion = pd.read_csv('Taller_2_Titulacion_Evaluación.csv', encoding='latin-1', delimiter=';')\n",
    "\n",
    "# Eliminar la columna \"Id\" si es necesario\n",
    "X_evaluacion = datos_evaluacion.drop('Id', axis=1)\n",
    "\n",
    "# Convertir variables categóricas en dummies si es necesario\n",
    "X_evaluacion = pd.get_dummies(X_evaluacion)\n",
    "\n",
    "# Hacer predicciones con el modelo cargado\n",
    "predictions = model.predict(X_evaluacion)\n",
    "\n",
    "# Convertir las predicciones a etiquetas legibles\n",
    "predicted_labels = np.where(predictions[:, 1], 'SÍ', 'NO')  # Si la segunda columna (índice 1) es True, entonces 'SÍ', de lo contrario 'NO'\n",
    "\n",
    "# Crear un DataFrame con las columnas Id y Prediccion\n",
    "resultados = pd.DataFrame({'Id': datos_evaluacion['Id'], 'Prediccion': predicted_labels})\n",
    "\n",
    "# Guardar el DataFrame como un archivo de texto separado por comas\n",
    "resultados.to_csv('clasificacion_titulo.txt', index=False, sep=',', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f0c6f-71f3-4282-a0b5-3364c16d3a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
